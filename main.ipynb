{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3690c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "main.py\n",
    "=======\n",
    "End-to-end training pipeline for the Taylor-expansion Seq2SeqLSTM model.\n",
    "\n",
    "Usage\n",
    "-----\n",
    "Just edit the CONFIG section below and run:\n",
    "    python main.py\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import tempfile\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from dataset import DualBPETokenizer, TaylorDataset\n",
    "from model import Seq2SeqLSTM\n",
    "from train_validate_model import (\n",
    "    build_criterion,\n",
    "    build_optimizer,\n",
    "    build_scheduler,\n",
    "    print_metrics,\n",
    "    train_epoch,\n",
    "    validate,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86a8aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIG  — edit everything here; do not change code below\n",
    "# ============================================================\n",
    "\n",
    "# --- Paths ---\n",
    "DATASET_JSON_PATH   = \"datasets/taylor_dataset_sample.json\"   # single JSON file\n",
    "TOKENIZER_SAVE_PATH = \"model/dual_bpe_tokenizer.pkl\"       # where to save/load tokenizer\n",
    "CHECKPOINT_DIR      = \"model/checkpoints\"                   # directory for saved models\n",
    "BEST_MODEL_PATH     = os.path.join(CHECKPOINT_DIR, \"best_model.pt\")\n",
    "\n",
    "# --- Data split ---\n",
    "VAL_RATIO   = 0.15     # fraction of data used for validation (e.g. 0.15 = 15 %)\n",
    "RANDOM_SEED = 42       # for reproducible splits\n",
    "\n",
    "# --- Tokenizer ---\n",
    "NUM_MERGES = 50       # BPE merge operations per side\n",
    "\n",
    "# --- Model architecture ---\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM    = 512\n",
    "NUM_LAYERS    = 2\n",
    "DROPOUT       = 0.3\n",
    "\n",
    "# --- Training ---\n",
    "BATCH_SIZE           = 32\n",
    "NUM_EPOCHS           = 50\n",
    "LEARNING_RATE        = 1e-3\n",
    "CLIP_GRAD            = 1.0       # max gradient norm (0 → disabled)\n",
    "VALIDATE_AFTER_EPOCH = 1         # run validation every N epochs\n",
    "\n",
    "# --- LR scheduler (ReduceLROnPlateau) ---\n",
    "SCHEDULER_FACTOR   = 0.5\n",
    "SCHEDULER_PATIENCE = 3\n",
    "SCHEDULER_MIN_LR   = 1e-6\n",
    "\n",
    "# --- Validation / generation ---\n",
    "BEAM_WIDTH            = 3\n",
    "MAX_GEN_LEN           = 128\n",
    "COMPUTE_FUNCTIONAL    = True     # set False to skip sympy / numerical metrics\n",
    "\n",
    "# --- Best-model selection criterion ---\n",
    "# Options: \"val_loss\" (lower is better) or any metric where HIGHER is better:\n",
    "#   \"token_acc\", \"sentence_acc\", \"strict_sentence_acc\", \"prefix_acc\"\n",
    "BEST_MODEL_METRIC   = \"val_loss\"\n",
    "BEST_MODEL_MINIMIZE = True       # True → lower is better; False → higher is better\n",
    "CREATE_NEW_TOKENIZER = True\n",
    "\n",
    "# --- Device ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e295db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def split_indices(n: int, val_ratio: float, seed: int):\n",
    "    \"\"\"Return (train_indices, val_indices) with a fixed random split.\"\"\"\n",
    "    indices = list(range(n))\n",
    "    random.seed(seed)\n",
    "    random.shuffle(indices)\n",
    "    val_size   = max(1, int(n * val_ratio))\n",
    "    train_size = n - val_size\n",
    "    return indices[:train_size], indices[train_size:]\n",
    "\n",
    "\n",
    "def make_subset_loader(\n",
    "    dataset: TaylorDataset,\n",
    "    indices,\n",
    "    batch_size: int,\n",
    "    shuffle: bool,\n",
    ") -> DataLoader:\n",
    "    subset = Subset(dataset, indices)\n",
    "    return DataLoader(\n",
    "        subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "    )\n",
    "\n",
    "\n",
    "def is_better(new_val: float, best_val: float, minimize: bool) -> bool:\n",
    "    if minimize:\n",
    "        return new_val < best_val\n",
    "    return new_val > best_val\n",
    "\n",
    "\n",
    "def initial_best_value(minimize: bool) -> float:\n",
    "    return float(\"inf\") if minimize else float(\"-inf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15289182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "  Device : cpu\n",
      "  Config : 50 epochs, batch=32, lr=0.001\n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_seed(RANDOM_SEED)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "device = torch.device(DEVICE)\n",
    "print(f\"\\n{'='*65}\")\n",
    "print(f\"  Device : {device}\")\n",
    "print(f\"  Config : {NUM_EPOCHS} epochs, batch={BATCH_SIZE}, lr={LEARNING_RATE}\")\n",
    "print(f\"{'='*65}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a00e24d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tokenizer] Fitting on 'datasets/taylor_dataset_sample.json' …\n",
      "[tokenizer] Saved to 'model/dual_bpe_tokenizer.pkl'\n",
      "[tokenizer] Input  vocab size : 82\n",
      "[tokenizer] Output vocab size : 206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Fit or load tokenizer\n",
    "# ------------------------------------------------------------------\n",
    "tokenizer = DualBPETokenizer(num_merges=NUM_MERGES)\n",
    "\n",
    "if os.path.exists(TOKENIZER_SAVE_PATH) and not CREATE_NEW_TOKENIZER:\n",
    "    print(f\"[tokenizer] Loading from '{TOKENIZER_SAVE_PATH}' …\")\n",
    "    tokenizer.load(TOKENIZER_SAVE_PATH)\n",
    "else:\n",
    "    print(f\"[tokenizer] Fitting on '{DATASET_JSON_PATH}' …\")\n",
    "    tokenizer.fit(DATASET_JSON_PATH)\n",
    "    tokenizer.save(TOKENIZER_SAVE_PATH)\n",
    "    print(f\"[tokenizer] Saved to '{TOKENIZER_SAVE_PATH}'\")\n",
    "\n",
    "print(f\"[tokenizer] Input  vocab size : {tokenizer.input_vocab_size}\")\n",
    "print(f\"[tokenizer] Output vocab size : {tokenizer.output_vocab_size}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4343357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.input_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65966da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data] Total samples  : 1000\n",
      "[data] Train samples  : 850\n",
      "[data] Val   samples  : 150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Load full dataset, then split into train / val\n",
    "# ------------------------------------------------------------------\n",
    "full_dataset = TaylorDataset(DATASET_JSON_PATH, tokenizer)\n",
    "N = len(full_dataset)\n",
    "\n",
    "train_idx, val_idx = split_indices(N, VAL_RATIO, RANDOM_SEED)\n",
    "print(f\"[data] Total samples  : {N}\")\n",
    "print(f\"[data] Train samples  : {len(train_idx)}\")\n",
    "print(f\"[data] Val   samples  : {len(val_idx)}\\n\")\n",
    "\n",
    "train_loader = make_subset_loader(full_dataset, train_idx, BATCH_SIZE, shuffle=True)\n",
    "val_loader   = make_subset_loader(full_dataset, val_idx,   BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab068917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[model] Trainable parameters : 7,535,822\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Build model\n",
    "# ------------------------------------------------------------------\n",
    "model = Seq2SeqLSTM(\n",
    "    input_vocab_size  = tokenizer.input_vocab_size,\n",
    "    output_vocab_size = tokenizer.output_vocab_size,\n",
    "    embedding_dim     = EMBEDDING_DIM,\n",
    "    hidden_dim        = HIDDEN_DIM,\n",
    "    num_layers        = NUM_LAYERS,\n",
    "    dropout           = DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"[model] Trainable parameters : {n_params:,}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4875dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. Build training components\n",
    "# ------------------------------------------------------------------\n",
    "pad_id    = tokenizer.output_tokenizer.pad_id\n",
    "criterion = build_criterion(pad_id)\n",
    "optimizer = build_optimizer(model, lr=LEARNING_RATE)\n",
    "scheduler = build_scheduler(\n",
    "    optimizer,\n",
    "    factor   = SCHEDULER_FACTOR,\n",
    "    patience = SCHEDULER_PATIENCE,\n",
    "    min_lr   = SCHEDULER_MIN_LR,\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5. Training loop\n",
    "# ------------------------------------------------------------------\n",
    "best_metric_val = initial_best_value(BEST_MODEL_MINIMIZE)\n",
    "history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0fcf1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Training ---\n",
    "BATCH_SIZE           = 32\n",
    "NUM_EPOCHS           = 50\n",
    "LEARNING_RATE        = 1e-3\n",
    "CLIP_GRAD            = 1.0       # max gradient norm (0 → disabled)\n",
    "VALIDATE_AFTER_EPOCH = 1         # run validation every N epochs\n",
    "\n",
    "# --- LR scheduler (ReduceLROnPlateau) ---\n",
    "SCHEDULER_FACTOR   = 0.5\n",
    "SCHEDULER_PATIENCE = 3\n",
    "SCHEDULER_MIN_LR   = 1e-6\n",
    "\n",
    "# --- Validation / generation ---\n",
    "BEAM_WIDTH            = 1\n",
    "MAX_GEN_LEN           = 16\n",
    "COMPUTE_FUNCTIONAL    = True     # set False to skip sympy / numerical metrics\n",
    "\n",
    "# --- Best-model selection criterion ---\n",
    "# Options: \"val_loss\" (lower is better) or any metric where HIGHER is better:\n",
    "#   \"token_acc\", \"sentence_acc\", \"strict_sentence_acc\", \"prefix_acc\"\n",
    "BEST_MODEL_METRIC   = \"val_loss\"\n",
    "BEST_MODEL_MINIMIZE = True       # True → lower is better; False → higher is better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c81af40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/50  |  train_loss=4.03401  |  4.5s\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  Epoch    1\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  Train loss                  : 4.034014\n",
      "  Val loss                    : 3.585264\n",
      "\n",
      "  Token accuracy              : 0.0503\n",
      "  Sentence accuracy           : 0.0000\n",
      "  Strict sentence accuracy    : 0.0000\n",
      "  Prefix accuracy             : 0.0638\n",
      "  Length accuracy             : 0.1067\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  ★ New best val_loss: 3.585264 ↓  → saved to 'model/checkpoints/best_model.pt'\n",
      "Epoch    2/50  |  train_loss=3.41286  |  4.0s\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  Epoch    2\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  Train loss                  : 3.412859\n",
      "  Val loss                    : 3.212266\n",
      "\n",
      "  Token accuracy              : 0.0542\n",
      "  Sentence accuracy           : 0.0000\n",
      "  Strict sentence accuracy    : 0.0000\n",
      "  Prefix accuracy             : 0.0621\n",
      "  Length accuracy             : 0.1533\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  ★ New best val_loss: 3.212266 ↓  → saved to 'model/checkpoints/best_model.pt'\n",
      "Epoch    3/50  |  train_loss=3.07516  |  3.9s\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  Epoch    3\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  Train loss                  : 3.075161\n",
      "  Val loss                    : 2.913295\n",
      "\n",
      "  Token accuracy              : 0.0503\n",
      "  Sentence accuracy           : 0.0000\n",
      "  Strict sentence accuracy    : 0.0000\n",
      "  Prefix accuracy             : 0.0621\n",
      "  Length accuracy             : 0.1533\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  ★ New best val_loss: 2.913295 ↓  → saved to 'model/checkpoints/best_model.pt'\n",
      "Epoch    4/50  |  train_loss=2.73686  |  4.1s\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  Epoch    4\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  Train loss                  : 2.736861\n",
      "  Val loss                    : 2.640056\n",
      "\n",
      "  Token accuracy              : 0.0410\n",
      "  Sentence accuracy           : 0.0000\n",
      "  Strict sentence accuracy    : 0.0000\n",
      "  Prefix accuracy             : 0.0316\n",
      "  Length accuracy             : 0.1267\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  ★ New best val_loss: 2.640056 ↓  → saved to 'model/checkpoints/best_model.pt'\n",
      "Epoch    5/50  |  train_loss=2.47932  |  4.2s\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  Epoch    5\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  Train loss                  : 2.479320\n",
      "  Val loss                    : 2.452916\n",
      "\n",
      "  Token accuracy              : 0.0384\n",
      "  Sentence accuracy           : 0.0000\n",
      "  Strict sentence accuracy    : 0.0000\n",
      "  Prefix accuracy             : 0.0325\n",
      "  Length accuracy             : 0.1333\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  ★ New best val_loss: 2.452916 ↓  → saved to 'model/checkpoints/best_model.pt'\n",
      "Epoch    6/50  |  train_loss=2.30751  |  4.1s\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  Epoch    6\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  Train loss                  : 2.307506\n",
      "  Val loss                    : 2.362307\n",
      "\n",
      "  Token accuracy              : 0.0675\n",
      "  Sentence accuracy           : 0.0067\n",
      "  Strict sentence accuracy    : 0.0000\n",
      "  Prefix accuracy             : 0.0543\n",
      "  Length accuracy             : 0.1600\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  ★ New best val_loss: 2.362307 ↓  → saved to 'model/checkpoints/best_model.pt'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m t0 = time.time()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# --- Train one epoch ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP_GRAD\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m elapsed = time.time() - t0\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>4d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  |  train_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  |  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gsoc/task/train_validate_model.py:151\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, optimizer, criterion, device, clip_grad)\u001b[39m\n\u001b[32m    148\u001b[39m B, T, V = logits.shape\n\u001b[32m    149\u001b[39m loss = criterion(logits.reshape(B * T, V), tgt_out.reshape(B * T))\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m clip_grad > \u001b[32m0\u001b[39m:\n\u001b[32m    153\u001b[39m     nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gsoc/task/.venv/lib/python3.13/site-packages/torch/_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gsoc/task/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gsoc/task/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "\n",
    "    # --- Train one epoch ---\n",
    "    train_loss = train_epoch(\n",
    "        model, train_loader, optimizer, criterion, device, CLIP_GRAD\n",
    "    )\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"Epoch {epoch:>4d}/{NUM_EPOCHS}  |  train_loss={train_loss:.5f}  |  {elapsed:.1f}s\", end=\"\")\n",
    "\n",
    "    # --- Validate every VALIDATE_AFTER_EPOCH epochs ---\n",
    "    if epoch % VALIDATE_AFTER_EPOCH == 0:\n",
    "        val_metrics = validate(\n",
    "            model,\n",
    "            val_loader,\n",
    "            tokenizer,\n",
    "            device,\n",
    "            criterion,\n",
    "            beam_width         = BEAM_WIDTH,\n",
    "            max_gen_len        = MAX_GEN_LEN,\n",
    "            compute_functional = COMPUTE_FUNCTIONAL,\n",
    "        )\n",
    "\n",
    "        print()  # newline after train line\n",
    "        print_metrics(epoch, train_loss, val_metrics)\n",
    "\n",
    "        # --- LR scheduler step (on val_loss) ---\n",
    "        scheduler.step(val_metrics[\"val_loss\"])\n",
    "\n",
    "        # --- Save best model ---\n",
    "        current_metric = val_metrics[BEST_MODEL_METRIC]\n",
    "        if is_better(current_metric, best_metric_val, BEST_MODEL_MINIMIZE):\n",
    "            best_metric_val = current_metric\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\":            epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state\":  optimizer.state_dict(),\n",
    "                    \"scheduler_state\":  scheduler.state_dict(),\n",
    "                    \"val_metrics\":      val_metrics,\n",
    "                    \"train_loss\":       train_loss,\n",
    "                    \"config\": {\n",
    "                        \"input_vocab_size\":  tokenizer.input_vocab_size,\n",
    "                        \"output_vocab_size\": tokenizer.output_vocab_size,\n",
    "                        \"embedding_dim\":     EMBEDDING_DIM,\n",
    "                        \"hidden_dim\":        HIDDEN_DIM,\n",
    "                        \"num_layers\":        NUM_LAYERS,\n",
    "                        \"dropout\":           DROPOUT,\n",
    "                    },\n",
    "                },\n",
    "                BEST_MODEL_PATH,\n",
    "            )\n",
    "            dir_sign = \"↓\" if BEST_MODEL_MINIMIZE else \"↑\"\n",
    "            print(\n",
    "                f\"  ★ New best {BEST_MODEL_METRIC}: \"\n",
    "                f\"{current_metric:.6f} {dir_sign}  → saved to '{BEST_MODEL_PATH}'\"\n",
    "            )\n",
    "\n",
    "        history.append({\"epoch\": epoch, \"train_loss\": train_loss, **val_metrics})\n",
    "    else:\n",
    "        print()   # close the train-only line\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6. Final summary\n",
    "# ------------------------------------------------------------------\n",
    "print(f\"\\n{'='*65}\")\n",
    "print(f\"  Training complete.\")\n",
    "print(f\"  Best {BEST_MODEL_METRIC} : {best_metric_val:.6f}\")\n",
    "print(f\"  Checkpoint      : {BEST_MODEL_PATH}\")\n",
    "print(f\"{'='*65}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce5574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_best_model(checkpoint_path: str, device: Optional[str] = None) -> Seq2SeqLSTM:\n",
    "    \"\"\"Load and return the best saved model from a checkpoint file.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> model = load_best_model(\"checkpoints/best_model.pt\")\n",
    "    >>> model.eval()\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "    cfg  = ckpt[\"config\"]\n",
    "    model = Seq2SeqLSTM(\n",
    "        input_vocab_size  = cfg[\"input_vocab_size\"],\n",
    "        output_vocab_size = cfg[\"output_vocab_size\"],\n",
    "        embedding_dim     = cfg[\"embedding_dim\"],\n",
    "        hidden_dim        = cfg[\"hidden_dim\"],\n",
    "        num_layers        = cfg[\"num_layers\"],\n",
    "        dropout           = cfg[\"dropout\"],\n",
    "    ).to(device)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    print(f\"[load] Loaded epoch {ckpt['epoch']} | \"\n",
    "          f\"val_loss={ckpt['val_metrics']['val_loss']:.5f}\")\n",
    "    return model\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
